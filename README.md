# **ğŸ“Œ Twitter Data Pipeline Using Apache Airflow & AWS**  

## **ğŸš€ Overview**  
This project builds an **automated data pipeline** that extracts **real-time tweets**, processes them using **Python**, orchestrates the workflow using **Apache Airflow**, and stores the data in **Amazon S3** for further analysis.  

![image](https://github.com/user-attachments/assets/db427031-2031-4a7a-b3eb-abac7cd3da4f)

ğŸ“Œ **Workflow Breakdown:**  
1ï¸âƒ£ **Extract tweets** from **Twitter API** using **Python**.  
2ï¸âƒ£ **Process & clean data** before storage.  
3ï¸âƒ£ **Use Apache Airflow (hosted on EC2)** to schedule and automate the pipeline.  
4ï¸âƒ£ **Store structured tweets in Amazon S3** for further processing or analytics.  

---

## **ğŸ›  Tech Stack**  
- **Programming Language**: Python  
- **Orchestration**: Apache Airflow (on AWS EC2)  
- **Cloud Storage**: Amazon S3  
- **API Integration**: Twitter API  

---

## **ğŸ“‚ Project Structure**  
```
â”œâ”€â”€ dags/                    # Airflow DAGs (Workflow Definitions)
â”‚   â”œâ”€â”€ twitter_pipeline.py  # DAG for fetching and storing tweets
â”‚   â”œâ”€â”€ config.py            # API keys and configuration
â”œâ”€â”€ scripts/                 # Python scripts for API extraction
â”‚   â”œâ”€â”€ fetch_tweets.py      # Extracts tweets from Twitter API
â”‚   â”œâ”€â”€ process_tweets.py    # Data cleaning and preprocessing
â”œâ”€â”€ requirements.txt         # Dependencies for Python & Airflow
â”œâ”€â”€ README.md                # Project documentation
â””â”€â”€ config/                  # Configuration files
```

---

## **ğŸ“Œ Workflow Explained**  
### **1ï¸âƒ£ Data Extraction**  
- The **Twitter API** is used to fetch live tweets based on specific keywords, hashtags, or user handles.  

### **2ï¸âƒ£ Data Processing**  
- The extracted tweet data is **cleaned** and formatted (removing special characters, filtering retweets, etc.).  

### **3ï¸âƒ£ Workflow Automation with Apache Airflow**  
- Apache Airflow (hosted on **AWS EC2**) schedules and manages the end-to-end pipeline.  
- Airflow ensures the pipeline runs **automatically at scheduled intervals** or **on demand**.  

### **4ï¸âƒ£ Storing Data in Amazon S3**  
- The **processed tweets** are saved in **Amazon S3** in JSON/CSV format for further analysis.  
- S3 enables **cost-effective** and **scalable storage** for large tweet datasets.  

---

## **ğŸš€ Setting Up the Project**  
### **1ï¸âƒ£ Prerequisites**  
âœ” AWS Account (for **EC2 & S3**)  
âœ” Twitter Developer Account (for **API keys**)  
âœ” Python 3.x installed  
âœ” Apache Airflow setup on EC2  

### **2ï¸âƒ£ Steps to Deploy**  
1. **Create an S3 bucket** to store the tweets.  
2. **Launch an EC2 instance** and install Apache Airflow.  
3. **Configure the Twitter API keys** in the project.  
4. **Deploy Airflow DAGs** to automate the pipeline.  

---

## **ğŸ“Œ Use Cases**  
âœ… **Social Media Analytics** â€“ Analyze trends, hashtags, and user sentiment.  
âœ… **Real-Time Monitoring** â€“ Track breaking news, public sentiment, and trending topics.  
âœ… **Data Storage & Future Processing** â€“ Store tweets for later analysis using **AWS Glue, Athena, or Redshift**.  

---

## **âš¡ Challenges & Solutions**  
ğŸ”¹ **Twitter API Rate Limits** â†’ Implemented **pagination & sleep intervals** between API calls.  
ğŸ”¹ **Handling Large Tweet Volumes** â†’ Used **batch processing** before writing to S3.  
ğŸ”¹ **Ensuring Airflow Reliability** â†’ Set up **retry policies** and logging for debugging failures.  

---
